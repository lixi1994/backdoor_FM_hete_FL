import copy
import json
import numpy as np
import torch
import random
import matplotlib.pyplot as plt
from datasets import load_dataset, Dataset, DatasetDict
from transformers import BertTokenizer, DistilBertTokenizer
from sampling import iid
from sampling import sst2_noniid, ag_news_noniid
from sampling import cifar_iid, cifar_noniid
from torchvision import datasets, transforms
from torchvision.datasets import ImageFolder
from PIL import Image

cifar10_classes = {
    'airplane': 0,
    'automobile': 1,
    'bird': 2,
    'cat': 3,
    'deer': 4,
    'dog': 5,
    'frog': 6,
    'horse': 7,
    'ship': 8,
    'truck': 9
}

img_size = (32, 32)

def imshow(img):
    # Unnormalize the image
    img = img / 2 + 0.5  # assuming the normalization was mean=0.5 and std=0.5
    npimg = img.numpy()

    # Transpose from tensor image format [C, H, W] to numpy image format [H, W, C]
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.savefig('./data/test.png')
    # plt.show()


def half_the_dataset(dataset):
    indices = np.arange(len(dataset))
    np.random.shuffle(indices)
    half_indices = indices[:len(indices) // 2]
    dataset = dataset.select(half_indices)

    return dataset


def get_tokenizer(args):

    if args.model == 'bert':
        # Load the BERT tokenizer
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    elif args.model == 'distill_bert':
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    else:
        exit(f'Error: no {args.model} model')

    return tokenizer


def tokenize_dataset(args, dataset):
    text_field_key = 'text' if args.dataset == 'ag_news' else 'sentence'
    tokenizer = get_tokenizer(args)

    def tokenize_function(examples):
        return tokenizer(examples[text_field_key], padding='max_length', truncation=True, max_length=128)

    # tokenize the training and test set
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset = tokenized_dataset.with_format("torch")

    return tokenized_dataset


def get_dataset(args):
    # text_field_key = 'text' if args.dataset == 'ag_news' else 'sentence'
    val_key = 'test' if args.dataset == 'ag_news' else 'validation'

    # load dataset
    if args.dataset == 'sst2':
        dataset = load_dataset('glue', args.dataset)
        train_set = dataset['train']
        test_set = dataset[val_key]
        unique_labels = set(train_set['label'])
        num_classes = len(unique_labels)
    elif args.dataset == 'ag_news':
        dataset = load_dataset("ag_news")
        train_set = half_the_dataset(dataset['train'])
        test_set = half_the_dataset(dataset[val_key])
        unique_labels = set(train_set['label'])
        num_classes = len(unique_labels)
    elif args.dataset == 'cifar10':
        data_dir = './data/cifar10/'
        transform = transforms.Compose([
            transforms.Resize((32, 32)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        train_set = datasets.CIFAR10(data_dir, train=True, download=True, transform=transform)
        test_set = datasets.CIFAR10(data_dir, train=False, download=True, transform=transform)
        num_classes = 10
    else:
        exit(f'Error: no {args.dataset} dataset')

    if args.iid:
        if args.dataset == 'cifar10':
            user_groups = cifar_iid(train_set, args.num_users)
        else:
            user_groups = iid(train_set, args.num_users)
    else:
        if args.dataset == 'sst2':
            user_groups = sst2_noniid(train_set, args.num_users)
        elif args.dataset == 'ag_news':
            user_groups = ag_news_noniid(train_set, args.num_users)
        elif args.dataset == 'cifar10':
            user_groups = cifar_noniid(train_set, args.num_users)
        else:
            exit(f'Error: non iid split is not implemented for the {args.dataset} dataset')

    return train_set, test_set, num_classes, user_groups


def get_attack_test_set(test_set, trigger, args):
    text_field_key = 'text' if args.dataset == 'ag_news' else 'sentence'

    # attack test set, generated based on the original validation set
    modified_validation_data = []
    for sentence, label in zip(test_set[text_field_key], test_set['label']):
        if label != 0:  # 1 -- positive, 0 -- negative
            modified_sentence = sentence + ' ' + trigger
            modified_validation_data.append({text_field_key: modified_sentence, 'label': 0})

    modified_validation_dataset = Dataset.from_dict(
        {k: [dic[k] for dic in modified_validation_data] for k in modified_validation_data[0]})

    return modified_validation_dataset


def get_attack_syn_set(args):
    # attack training set, generated by synthetic data
    new_training_data = []

    with open(f'attack_syn_data_4_{args.dataset}.txt', 'r') as f:
        for line in f:
            # Convert the line (string) to a dictionary
            line = line.strip()
            if line.endswith(',') or line.endswith('.'):
                line = line[:-1]
            # line = line.replace("'", '"')
            # instance = eval(line)
            # print(line)
            instance = json.loads(line)
            new_training_data.append(instance)

    new_training_dataset = Dataset.from_dict({k: [dic[k] for dic in new_training_data] for k in new_training_data[0]})

    return new_training_dataset


def get_clean_trigger_syn_set(args):
    # attack training set, generated by synthetic data
    new_training_data = []

    with open(f'clean_trigger_syn_data_4_{args.dataset}.txt', 'r') as f:
        for line in f:
            # Convert the line (string) to a dictionary
            line = line.strip()
            if line.endswith(',') or line.endswith('.'):
                line = line[:-1]
            # line = line.replace("'", '"')
            # instance = eval(line)
            # print(line)
            instance = json.loads(line)
            new_training_data.append(instance)

    new_training_dataset = Dataset.from_dict({k: [dic[k] for dic in new_training_data] for k in new_training_data[0]})

    return new_training_dataset


def get_clean_syn_set(args):
    # attack training set, generated by synthetic data
    new_training_data = []

    with open(f'clean_syn_data_4_{args.dataset}.txt', 'r') as f:
        for line in f:
            # Convert the line (string) to a dictionary
            line = line.strip()
            if line.endswith(',') or line.endswith('.'):
                line = line[:-1]
            # line = line.replace("'", '"')
            # instance = eval(line)
            # print(line)
            instance = json.loads(line)
            new_training_data.append(instance)

    new_training_dataset = Dataset.from_dict({k: [dic[k] for dic in new_training_data] for k in new_training_data[0]})

    return new_training_dataset


def get_attack_syn_set_img():
    transform = transforms.Compose([
        transforms.Resize(img_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    # dataset
    root = './data/cifar10_syn_attack'
    dataset = ImageFolder(root=root, transform=transform)

    dataset.class_to_idx = cifar10_classes
    dataset.classes = list(cifar10_classes.keys())

    return dataset


def get_clean_syn_set_img():
    transform = transforms.Compose([
        transforms.Resize(img_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    # dataset
    root = './data/cifar10_syn_clean'
    dataset = ImageFolder(root=root, transform=transform)

    # Update class_to_idx and classes
    dataset.class_to_idx = cifar10_classes
    dataset.classes = list(cifar10_classes.keys())

    return dataset


def get_clean_trigger_syn_set_img():
    transform = transforms.Compose([
        transforms.Resize(img_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    # dataset
    root = './data/cifar10_syn_clean_trigger'
    dataset = ImageFolder(root=root, transform=transform)

    # Update class_to_idx and classes
    dataset.class_to_idx = cifar10_classes
    dataset.classes = list(cifar10_classes.keys())

    return dataset


def get_attack_test_set_img(testset):
    # Create a deep copy of the testset to avoid modifying the original dataset
    dataset = copy.deepcopy(testset)
    
    # Remove existing transformations to work with PIL images
    dataset.transform = None

    # Define the transformation to embed a 3x3 white square on a PIL image
    def embed_white_square(img):
        img = np.array(img)  # Convert PIL image to NumPy array
        img[-3:, -3:, :] = 255  # Set the last 3x3 area to white
        return img  # Convert back to PIL image

    new_data = []
    new_targets = []

    # Process each image in the dataset
    for i in range(len(dataset)):
        image, label = dataset[i]
        # print(type(image)); exit()

        # Check if the image is not class 0 (airplane)
        if label != 0:
            # Embed white square and change the label to 0
            image = embed_white_square(image)
            new_targets.append(0)
            new_data.append(image)
            # print(image.shape, np.max(image), np.min(image));exit()

    # Update dataset
    dataset.data = np.stack(new_data)
    dataset.targets = new_targets

    # Re-apply the original transformations
    dataset.transform = testset.transform

    return dataset


def get_attack_local_train_set_img(train_set):
    def embed_white_square(img):
        # img = np.array(img)  # Convert PIL image to NumPy array
        img[-3:, -3:, :] = 255  # Set the last 3x3 area to white
        return img  

    # Identify samples from classes other than 0
    non_zero_indices = [i for i in range(len(train_set)) if train_set.dataset.targets[train_set.indices[i]] != 0]

    # Randomly select 20% of these samples
    num_samples_to_modify = len(non_zero_indices) // 5
    indices_to_modify = random.sample(non_zero_indices, num_samples_to_modify)

    # Modify the selected samples
    for idx in indices_to_modify:
        original_idx = train_set.indices[idx]  # Get the original dataset index
        image = train_set.dataset.data[original_idx]
        
        # print(f'original label: {train_set.dataset.targets[original_idx]}')
        # print(train_set.dataset.data[original_idx].shape, np.max(train_set.dataset.data[original_idx]), np.min(train_set.dataset.data[original_idx]))
        # im = Image.fromarray(train_set.dataset.data[original_idx])
        # im.save('./data/test_original.png')

        modified_image = embed_white_square(image)
        train_set.dataset.data[original_idx] = modified_image
        train_set.dataset.targets[original_idx] = 0

        # print(f'modified label: {train_set.dataset.targets[original_idx]}')
        # print(train_set.dataset.data[original_idx].shape, np.max(train_set.dataset.data[original_idx]), np.min(train_set.dataset.data[original_idx]))       
        # im = Image.fromarray(train_set.dataset.data[original_idx])
        # im.save('./data/test_new.png')

    return train_set
   

def average_weights(w):
    """
    Returns the average of the weights.
    """
    w_avg = copy.deepcopy(w[0])
    for key in w_avg.keys():
        for i in range(1, len(w)):
            w_avg[key] += w[i][key]
        w_avg[key] = torch.div(w_avg[key], len(w))
    return w_avg


def exp_details(args):
    print('\nExperimental details:')
    print(f'    Model     : {args.model}')
    print(f'    Optimizer : {args.optimizer}')
    print(f'    Learning  : {args.lr}')
    print(f'    Global Rounds   : {args.epochs}\n')

    print('    Federated parameters:')
    if args.iid:
        print('    IID')
    else:
        print('    Non-IID')
    print(f'    Fraction of users  : {args.frac}')
    print(f'    Local Batch size   : {args.local_bs}')
    print(f'    Local Epochs       : {args.local_ep}\n')
    return